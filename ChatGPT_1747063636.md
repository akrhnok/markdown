### ChatGPTが人間のように例と「記憶」を用いて言語を生成する

#### 主要ポイント:
- ChatGPTのような大規模言語モデル（LLM）は、人間のように類推を用いて言語パターンを一般化する。
- これらのモデルは、厳格な文法規則ではなく、保存された例や類推に依存している。
- 研究では、GPT-Jが未知の形容詞を名詞に変換する際に、人間の判断と類似した行動を示した。
- 牛津大学とAllen Institute for AIの研究チームがこの研究を主導し、PNASに発表した。

#### 詳細な解説:
牛津大学とAllen Institute for AI（Ai2）の研究チームが主導した新しい研究によれば、ChatGPTのような大規模言語モデル（LLM）は、人間のように類推を用いて言語パターンを一般化することが明らかになりました。この研究は、LLMが訓練データから規則を推論することで言語を生成すると広く考えられている仮定に挑戦しています。代わりに、これらのモデルは保存された例に依存し、未知の単語を扱う際に人間のように類推を行います。

研究チームは、GPT-J（EleutherAIが2021年に開発したオープンソースの大規模言語モデル）を用いて、英語で非常に一般的な単語形成パターンを調査しました。このパターンは、形容詞を名詞に変換するために接尾辞「-ness」または「-ity」を追加するものです。例えば、「happy」は「happiness」になり、「available」は「availability」になります。研究チームは、LLMが以前に遭遇したことのない200の架空の英語の形容詞（例：cormasive、friquish）を生成し、GPT-Jにそれぞれを名詞に変換するよう依頼しました。GPT-Jの応答は、人間の選択や、規則を用いて一般化するモデルと、保存された例に基づく類推を用いるモデルの予測と比較されました。

結果は、LLMの行動が人間の類推的推論に似ていることを示しました。GPT-Jは規則を用いるのではなく、訓練中に「見た」実際の単語との類似性に基づいて回答しました。これは、人々が新しい単語について考える際に行うことと非常に似ています。例えば、「friquish」は「friquishness」ではなく「friquishity」に変換されました。

#### まとめ:
この研究は、ChatGPTのような大規模言語モデルが、人間のように例と「記憶」を用いて言語を生成することを示しています。これは、LLMが厳格な文法規則ではなく類推を用いて言語パターンを一般化するという新たな理解をもたらします。この発見は、AIの言語生成能力に関する私たちの理解を深めるものであり、将来的にはより自然な対話システムの開発に貢献する可能性があります。

#### 元記事へのリンク:
[Like humans, ChatGPT favours examples and ‘memories’ – not rules – to generate language | University of Oxford](https://www.ox.ac.uk/news/2025-05-12-humans-chatgpt-favours-examples-and-memories-not-rules-generate-language)

**元記事:** [Like humans, ChatGPT favours examples and ‘memories’ – not rules – to generate language University of Oxford](https://www.ox.ac.uk/news/2025-05-12-humans-chatgpt-favours-examples-and-memories-not-rules-generate-language)