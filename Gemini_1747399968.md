### GoogleがAndroidとChromeの新しいAIツールを発表、アクセシビリティ向上へ

#### 主要ポイント:
- GoogleがAndroidとChromeのAIアクセシビリティツールを更新
- Gemini AIがTalkBackに統合され、画像やスクリーン上のコンテンツに関する質問が可能に
- Expressive Captionsが音声のニュアンスを捉えたライブキャプションを提供
- Project Euphoniaを通じて、非典型的な発声パターンを持つ人々のためのスピーチ認識をグローバルに拡大
- 学生向けのアクセシビリティ機能が強化

#### 詳細な解説:

Googleは、Global Accessibility Awareness Dayを記念して、AndroidとChromeの新しいAIアクセシビリティツールを発表しました。これらのツールは、視覚や聴覚に障害を持つユーザーのためのモバイル機能を強化することを目指しています。特に、Gemini AIがTalkBackに統合され、画像やスクリーン上のコンテンツに関する質問に答えることができるようになりました。例えば、ギターの写真が共有された場合、その種類や色について質問することが可能です。また、ショッピングアプリを使用している際には、商品の素材や割引情報についても尋ねることができます。

さらに、Expressive Captionsという新機能が導入され、多くのAndroidアプリで音声のニュアンスを捉えたライブキャプションを生成します。この機能は、伸ばした音声（例えば「nooooo」や「amaaazing shot」）や口笛、喉を鳴らす音なども認識します。現在、この機能は英語で提供されており、Android 15以降を搭載したデバイスを持つアメリカ、イギリス、カナダ、オーストラリアのユーザーが利用できます。

Googleはまた、2019年に開始したProject Euphoniaを通じて、非典型的な発声パターンを持つ人々のためのスピーチ認識をグローバルに拡大しています。開発者向けには、Project EuphoniaのGitHubページでオープンソースのリソースが提供されており、個別の音声ツールや多様な発声パターンに対応したスピーチ認識モデルを構築することができます。また、Google.orgとUniversity College Londonが共同で設立したCentre for Digital Language Inclusion (CDLI)は、10のAfrican言語のオープンデータセットを開発し、スピーチ認識技術を構築しています。

#### まとめ:
Googleは、AndroidとChromeの新しいAIツールを発表し、アクセシビリティを大幅に向上させました。Gemini AIの統合やExpressive Captionsの導入、Project Euphoniaのグローバル展開など、さまざまな取り組みが進められています。これらの機能は、視覚や聴覚に障害を持つユーザーや学生にとって大きな助けとなるでしょう。

#### 元記事へのリンク:
[Google rolls out new AI tools to improve accessibility on Android and Chrome](https://www.fonearena.com/blog/423547/google-ai-tools-accessibility-android-chrome.html)

**元記事:** [Google rolls out new AI tools to improve accessibility on Android and Chrome](https://www.fonearena.com/blog/453766/google-new-ai-tools-android-chrome.html)