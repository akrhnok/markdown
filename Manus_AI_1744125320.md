# DeepSeekと清華大学による自己改善型AIモデル共同開発：概要と展望

## 1. はじめに

本レポートは、中国のAI企業DeepSeek（ディープシーク）と清華大学が共同で開発を進めている自己改善型AIモデルに関するニュース記事を基に、その概要と意義、今後の展望について分析するものである。本モデルは、AIモデルの運用コスト削減を目的とし、強化学習（Reinforcement Learning: RL）を活用した新たなアプローチを採用している。

## 2. 開発の背景と目的

### 2.1. 運用コスト削減の必要性

AIモデル、特に大規模言語モデル（LLM）の運用には、膨大な計算リソースとコストが必要となる。DeepSeekは、この課題を解決するため、AIモデルに必要なトレーニング量を削減し、効率性を高めることを目指している。

### 2.2. 強化学習（RL）の活用

DeepSeekと清華大学は、人間の試行錯誤プロセスを模倣する強化学習（RL）に着目した。RLは、AIモデルがより正確で理解しやすい回答を生成した場合に報酬を与えることで、モデルの性能を向上させる。

## 3. 自己改善型AIモデル「DeepSeek-GRM」の詳細

### 3.1. 「自己原理批判チューニング（SPCT）」

DeepSeekは、RLの課題を解決するために、「自己原理批判チューニング（Self-Principled Critique Tuning＝SPCT）」と呼ばれる手法を開発した。この手法により、少ない計算リソースで高いパフォーマンスを発揮することを目指している。

### 3.2. 「DeepSeek-GRM」の概要

DeepSeekは、この新モデルを「DeepSeek-GRM」（GRMは汎用（はんよう）報酬モデリング＝generalist reward modelingの略）と名付け、オープンソースで公開する予定である。

### 3.3. 他のAI企業との比較

中国のアリババグループや米オープンAIも、AIモデルの自己改善能力向上に取り組んでおり、DeepSeekの取り組みは、AI分野における競争激化を示唆している。

## 4. 技術的側面と他社との比較

### 4.1. 混合エキスパート（MoE）アーキテクチャ

DeepSeekのモデルは、リソースを効率的に活用するために、混合エキスパート（Mixture of Experts: MoE）アーキテクチャに大きく依存している。MoEは、複数の専門的なモデルを組み合わせることで、計算効率を高める手法である。

### 4.2. 主要なAIモデルとの比較

| モデル | 開発元 | 特徴 

**元記事:** [DeepSeekと清華大、自己改善型ＡＩモデルを共同開発中(TBS CROSS DIG with Bloomberg) - goo ニュース](https://news.goo.ne.jp/article/tbs_bb/business/tbs_bb-1840629.html)