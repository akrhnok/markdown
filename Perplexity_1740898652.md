# Microsoft AIのLongRoPE2: 大規模言語モデルのコンテキストウィンドウを128Kトークンまで拡張する革新的な手法

## 緒言

本レポートでは、Microsoft AIが開発したLongRoPE2について、その概要と技術的詳細を解説します。LongRoPE2は、大規模言語モデル（LLM）のコンテキストウィンドウを128Kトークンまで拡張し、短いコンテキストでの精度を97%以上維持する新しい手法です。

## 大規模言語モデルのコンテキストウィンドウの課題

大規模言語モデルは、長いコンテキストシーケンスを効果的に処理することが難しいという課題があります。例えば、GPT-4oやLLaMA3.1は128Kトークンまでのコンテキストウィンドウをサポートしていますが、長いシーケンスでの高性能を維持するのは困難です。Rotary Positional Embeddings（RoPE）は位置情報をエンコードする手法ですが、訓練範囲を超えて使用すると、分布外（OOD）の問題が発生し、パフォーマンスが低下します。

## LongRoPE2の概要

LongRoPE2は、以下の3つの主要な問題を解決するために設計されました：

1. **高次元RoPEの訓練不足**：高次元のRoPEが十分に訓練されていないため、長いトークン位置でOOD値が発生する問題を解決します。
2. **進化的検索に基づくRoPEの再スケーリング**：理論的な仮定を超えて再スケーリング係数を最適化し、長いコンテキストに適応します。
3. **混合コンテキストウィンドウの訓練**：短いシーケンスと長いシーケンスの両方でモデルを微調整し、短いコンテキストでのパフォーマンス低下を防ぎます。

## LongRoPE2の技術的アプローチ

LongRoPE2は、以下の手順でコンテキストウィンドウを拡張します：

1. **RoPEの真の重要次元の特定**：理論的な重要次元が実際のスケーリングニーズを過小評価していることを発見し、適応的な再スケーリング方法を開発しました。
2. **進化的検索による再スケーリング**：トークンごとのパープレキシティ評価に基づいて再スケーリング係数を動的に調整し、長いコンテキストでの効果を最大化します。
3. **混合コンテキストウィンドウの訓練**：短いシーケンスと長いシーケンスの両方でモデルを訓練し、短いコンテキストでの精度を97.6%以上維持します。

## パフォーマンス評価

LongRoPE2は、さまざまなベンチマークで優れたパフォーマンスを示しました。例えば、RULERベンチマークでは、LLaMA3-8Bを128Kトークンまで拡張し、82.03%のスコアを達成しました。これは、LongRoPEの73.40%やYaRNの49.39%を上回る結果です。また、Phi3-mini-3.8Bでも128Kトークンで58.81%のスコアを達成し、NTKの49.37%を大幅に上回りました。

さらに、LongRoPE2は、Needle in a Haystackテストでほぼ完璧な精度を達成し、長いシーケンス内で深く埋め込まれた情報を効果的に取得できました。

## 研究の主なポイント

- LongRoPE2は、LLaMA3-8Bを128Kトークンまで拡張し、82.03%の精度を達成しました。
- Metaの方法が800Bの訓練トークンを必要としたのに対し、LongRoPE2は10Bのトークンで同等の拡張を実現し、80倍の効率を達成しました。
- 短いコンテキストでのパフォーマンスを97.6%以上維持しました。
- 進化的検索による適応的なスケーリングが、静的な再スケーリング手法よりも優れていることが示されました。

## 結論

LongRoPE2は、大規模言語モデルのコンテキストウィンドウを128Kトークンまで拡張し、短いコンテキストでの精度を97%以上維持する革新的な手法です。この手法は、長いコンテキストシーケンスの処理における根本的な制約を解決し、AIアプリケーションの多様なニーズに対応する可能性を秘めています。

**元記事:** [Microsoft AI Released LongRoPE2 A Near-Lossless Method to Extend Large Language Model Context Windows to 128K Tokens While Retaining Over 97% Short-Context Accuracy - MarkTechPost](https://www.marktechpost.com/2025/03/01/microsoft-ai-released-longrope2-a-near-lossless-method-to-extend-large-language-model-context-windows-to-128k-tokens-while-retaining-over-97-short-context-accuracy/)