# ハッカーがAIを利用してAIを攻撃、新手法「Fun-Tuning」が脅威に

## 緒言

本レポートでは、最近のニュース記事を基に、ハッカーが人工知能（AI）を利用して他のAIを攻撃する新たな手法「Fun-Tuning」について、その概要と影響を客観的に分析する。

## AIによるAI攻撃の現状

### プロンプトインジェクション攻撃とは

プロンプトインジェクション攻撃は、大規模言語モデル（LLM）を操作するための方法の一つです。この攻撃では、AIが読み取るテキストに悪意のある指示を隠すことで、AIの元々のルールを無視させます。これにより、プライベートデータの漏洩や誤った回答の提供など、意図しない行動を引き起こすことが可能です。

### 従来の攻撃の課題

従来のプロンプトインジェクション攻撃は、特にGPT-4やGeminiなどのクローズドウェイトモデルに対しては、手動での試行錯誤が必要で、成功率が低いという課題がありました。

## 新手法「Fun-Tuning」の概要

### Fun-Tuningの開発と効果

大学の研究チームが開発した「Fun-Tuning」は、GoogleのGeminiのファインチューニングAPIを利用して、自動的に高成功率のプロンプトインジェクション攻撃を作成する手法です。この手法は、Geminiのトレーニングインターフェースを悪用し、攻撃の成功率を大幅に向上させます。テストでは、従来の攻撃方法と比較して、成功率が82%まで向上しました。

### Fun-Tuningの仕組み

Fun-Tuningは、トレーニングプロセスでの微妙な手がかりを利用し、攻撃の精度を高めます。具体的には、トレーニングエラーに対するモデルの反応をフィードバックとして活用し、攻撃の効果を最大化します。

## Fun-Tuningの影響と対策

### 攻撃の拡散性

Fun-Tuningで開発された攻撃は、Geminiの異なるバージョン間でも容易に転用可能です。これにより、一人の攻撃者が複数のプラットフォームにわたって攻撃を展開することが可能になります。

### 攻撃のコスト

GoogleがファインチューニングAPIを無料で提供しているため、Fun-Tuningによる攻撃のコストは非常に低く、わずか10ドルの計算時間で実行可能です。

### Googleの対応

Googleはこの脅威を認識していますが、ファインチューニング機能を変更するかどうかについてはコメントしていません。

### 対策の難しさ

研究者たちは、この種の攻撃に対する防御は簡単ではないと警告しています。トレーニングプロセスから重要なデータを削除すると、開発者にとってツールの有用性が低下しますが、そのままにしておくと攻撃者が利用しやすくなります。

## 結論

AIによるAI攻撃は新たな段階に入っており、AIが攻撃の対象だけでなく武器としても使われる時代が到来しています。Fun-Tuningのような新手法は、AIのセキュリティ対策がますます重要になることを示しています。

**元記事:** [Hackers are now using AI to break AI - and it’s working](https://bgr.com/tech/hackers-are-now-using-ai-to-break-ai-and-its-working/)