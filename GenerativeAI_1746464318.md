## AIが抱える偏見：ムスリムやアジア人をステレオタイプ化する問題

### 記事の主要ポイント

* 生成AIツール（ChatGPT、Grokなど）が、ムスリムやアジア人をステレオタイプ的に描写し、偏見を助長していると批判されている。
* この偏見は、AIが学習するデータセットや社会的な偏見に起因している。
* 「テロリスト」のイメージ生成において、GrokやChatGPTがケフィエ（中東で広く着用されるスカーフ）を着用した人物を描写するなど、特定のステレオタイプを反映した結果が出ている。
* AIのデータインデックスの手順は中立ではなく、既存の経済的・政治的覇権を反映している。
* AIは、性別、民族、人種に関する侮辱的な内容を含む1億以上のデータセットで訓練されており、これが差別的なアルゴリズムを形成している。

### 詳細解説

生成AIの普及が進むにつれて、その描写する人種、性別、民族に関する偏見が問題視されています。特に、ChatGPTやGrokといったツールが、ムスリムやアジア人を「オリエンタリズム」的な視点からステレオタイプ的に描写しているという批判が上がっています。

例えば、Grokに「テロリスト」のイメージを生成させたところ、ケフィエを着用した人物が描かれました。これは、AIが学習したデータの中に、中東やイスラム教に関連するステレオタイプ的なイメージが強く存在することを示唆しています。エディンバラ大学のShoaib Ahmed Malik博士は、「AIシステムは、メディア、安全保障に関する言説、政治的物語に存在する支配的な視覚的および概念的な関連性をエンコードしたデータに基づいて訓練されている」と指摘しています。

AIが偏見を持つ原因は、学習データにあります。AIは、性別、民族、人種に関する侮辱的な内容を含む膨大なデータセットで訓練されます。これらのデータは、ソーシャルメディアの言説などから収集され、差別的なアルゴリズムを形成します。このため、AIはステレオタイプを助長し、デジタルな排除を加速させる可能性があります。

| 問題点 | 説明 

**元記事:** [Caught in algorithm Muslims, Asians, and the biases of artificial intelligence](https://trt.global/world/article/831781fa517f)