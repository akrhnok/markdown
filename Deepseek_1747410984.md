### DeepSeekがV3の大規模モデル訓練のコスト削減方法を新論文で公開

#### 主要ポイント:
- DeepSeek-V3は2,048台のH800 GPUだけで効率的な訓練と推論を実現
- 4つの主要なイノベーション：MLAによるメモリ最適化、MoEとFP8精度による計算の節約、多面ネットワークトポロジーによる通信の改善、MTPによる高速な推論
- MLAによりKVキャッシュのメモリ使用量が1トークンあたり70KBに削減
- MoEアーキテクチャにより、671億パラメータのうち37億パラメータのみがアクティブ化され、訓練コストが90%削減
- FP8訓練により、計算とメモリ使用量が半分に

#### 詳細な解説:
DeepSeekは最新の論文で、DeepSeek-V3の大規模言語モデルがどのようにして効率的な訓練と推論を実現しているかを詳細に説明しています。通常、数万台のGPUが必要とされる中、DeepSeek-V3はわずか2,048台のH800 GPUでこれを達成しました。この効率性は、4つの主要なイノベーションによるものです。

まず、多頭潜在注意（MLA）により、KVキャッシュのメモリ使用量が1トークンあたり70KBに削減されました。これは競合他社のモデルと比較して最大で1/7のメモリ使用量です。次に、専門家混合（MoE）アーキテクチャとFP8精度の組み合わせにより、計算とメモリの使用量が大幅に削減されました。MoEにより、671億パラメータのうち37億パラメータのみが各フォワードパスでアクティブ化され、訓練コストが90%削減されました。さらに、FP8訓練により、計算とメモリ使用量が半分に抑えられ、精度への影響は最小限に抑えられました。

また、多面ネットワークトポロジーによる通信の改善と、多トークン予測（MTP）による高速な推論も重要な役割を果たしています。これらの技術的進歩により、DeepSeek-V3は他の大規模モデルと比較して非常に効率的であると言えます。

#### まとめ:
DeepSeekの新論文は、DeepSeek-V3の大規模言語モデルがどのようにして効率的な訓練と推論を実現しているかを明らかにしました。4つの主要なイノベーションにより、通常必要とされるGPUの数を大幅に削減し、訓練コストを90%削減することができました。これらの成果は、AIモデルの開発における新たな可能性を示しています。

#### 元記事へのリンク:
[DeepSeek reveals cost-cutting methods for V3 large model training in new paper · TechNode](https://technode.com/2025/05/16/deepseek-reveals-cost-cutting-methods-for-v3-large-model-training-in-new-paper/)

**元記事:** [DeepSeek reveals cost-cutting methods for V3 large model training in new paper · TechNode](https://technode.com/2025/05/16/deepseek-reveals-cost-cutting-methods-for-v3-large-model-training-in-new-paper/)