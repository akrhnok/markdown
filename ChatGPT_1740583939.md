# ChatGPTに絶対に言ってはいけないこと：個人情報のリスクと対策

## 緒言

本レポートでは、PCWorldの記事「Never say these things to ChatGPT. It could come back to bite you」を基に、ChatGPTやその他のAIチャットボットに個人情報を提供することのリスクと、それに対する対策について客観的に分析します。

## AIチャットボットのリスク

### データの収集と利用

AIチャットボットは、ユーザーとの会話を通じて得た情報をサーバーに保存し、それを利用して自身のモデルを改善します。例えば、OpenAIのChatGPTは、利用規約において「We may use the data you provide us to improve our models」と明記しており、ユーザーが提供したデータを利用する権利を保持しています。このため、ユーザーがチャットボットに共有した個人情報（金融情報、パスワード、住所、電話番号など）は、保存され、後で再利用される可能性があります。

### データの漏洩リスク

2023年5月に発生したChatGPTのデータ漏洩事件では、約101,000人のユーザーの個人情報が盗まれました。この事件は、AIチャットボットが保存するデータが外部からの攻撃に対して脆弱であることを示しています。また、企業でも同様の問題が発生しており、サムスンは自社のエンジニアがChatGPTに機密ソースコードをアップロードしたことを受けて、業務での使用を禁止しました。

## 対策と意識

### プライバシー設定の利用

ChatGPTには、チャット履歴を保存しない設定を有効にするプライバシー機能があります。この設定を利用することで、個人情報の保存を防ぐことができます。ユーザーはこの機能を積極的に利用し、個人情報の共有を最小限に抑えることが推奨されます。

### 情報共有の自制

AIチャットボットはアルゴリズムであり、友人や信頼できる存在ではありません。ユーザーはこの点を理解し、必要以上に個人情報を共有しないように注意する必要があります。特に、金融情報やパスワードなどの機密情報は絶対に共有しないことが重要です。

## 政府と業界の動き

### 規制とガイドライン

AIチャットボットのデータセキュリティに対する意識は高まっており、2023年10月30日にはアメリカのジョー・バイデン大統領が「Safe, Secure, and Trustworthy Development and use of Artificial Intelligence」に関する大統領令に署名しました。この文書では、AIシステムがプライバシーを尊重し、個人データを保護する必要性が強調されています。しかし、具体的な実施方法や法的枠組みはまだ明確になっておらず、AI企業による解釈に委ねられる部分もあります。

## 結論

AIチャットボットは便利なツールですが、個人情報のリスクを伴います。ユーザーはプライバシー設定を利用し、情報共有を自制することでリスクを軽減できます。また、政府や業界がデータセキュリティの強化に向けた取り組みを進めることで、より安全なAI利用環境が整備されることが期待されます。

**元記事:** [Never say these things to ChatGPT. It could come back to bite you PCWorld](https://www.pcworld.com/article/2535401/never-say-these-things-to-chatgpt-it-could-come-back-to-bite-you.html)