### 見出し: 科学研究の要約でチャットボットが過大評価、研究結果

#### 主要ポイント:
- 大規模言語モデル（LLM）は科学研究の要約で最大73%のケースで不正確な結論を出す。
- 正確さを求めるプロンプトが逆効果となり、過大評価を増加させる。
- 新しいLLMは古いモデルよりもパフォーマンスが悪い。
- チャットボットは人間の要約に比べて5倍も広範な一般化を行う傾向がある。
- 研究者は、科学的要約の信頼性を高めるための対策を提案。

#### 詳細な解説:
ウトレヒト大学のウウェ・ピーターズとウェスタン大学およびケンブリッジ大学のベンジャミン・チン・イーが行った研究によると、大規模言語モデル（LLM）であるChatGPTやDeepSeekなどのチャットボットは、科学研究の要約において不正確な結論を出すことが多いことが明らかになりました。この研究では、NatureやScience、The Lancetなどのトップジャーナルからの論文の要約を10の主要なLLMで評価し、4,900件の要約を収集しました。その結果、10のモデル中6つがオリジナルのテキストに含まれる主張を過大評価することがわかりました。例えば、「この研究では治療が有効であった」という慎重な過去形の主張が、「治療は有効である」という広範で現在形の主張に変換されるなど、微妙だが影響力のある方法で行われています。

さらに驚くべきことに、正確さを求めるプロンプトが逆効果となり、過大評価の問題を増加させることが判明しました。ピーターズは、「学生、研究者、政策立案者は、ChatGPTに不正確さを避けるように指示すれば、より信頼性の高い要約が得られると考えがちですが、我々の研究結果はその逆を証明しています」と述べています。また、チャットボットの要約と人間の要約を比較したところ、チャットボットは人間の要約に比べて5倍も広範な一般化を行う傾向があることがわかりました。新しいAIモデルは古いモデルよりもパフォーマンスが悪いという結果も出ています。

研究者は、科学的要約の信頼性を高めるための対策を提案しています。例えば、Claudeのような一般化の精度が高いLLMを使用すること、チャットボットの「温度」を低く設定すること（チャットボットの「創造性」を制限するパラメータ）、そして科学的要約では間接的かつ過去形の報告を強制するプロンプトを使用することが推奨されています。

#### まとめ:
この研究は、科学研究の要約においてチャットボットが過大評価を行う傾向があることを示しており、正確さを求めるプロンプトが逆効果となる可能性があることを警告しています。新しいAIモデルが古いモデルよりもパフォーマンスが悪いという結果も示唆されており、科学的要約の信頼性を高めるための対策が必要であることが強調されています。

#### 元記事へのリンク:
[Prominent chatbots routinely exaggerate science findings, study shows](リンク先URL)

**元記事:** [Prominent chatbots routinely exaggerate science findings, study shows](https://phys.org/news/2025-05-prominent-chatbots-routinely-exaggerate-science.html)